### Что нужно сделать:
Давайте реализуем классическое приложение для асинхронных задач -- интернет краулер. 
Принцип работы такой: на вход краулеру дается ссылка или ссылки. Далее он запрашивает контент этих страниц, парсит их, достает оттуда все доступные ссылки и уже идет по ним. Иначе говоря, работает рекурсивно. Нас будут интересовать только ссылки на внешние ресурсы.
Приложение необходимо реализовать используя асинхронный python и с помощью модуля aiohttp. Сделайте так, чтобы количество итераций можно было сконфигурировать. По дефолту пусть это будет 3. 
Все ссылки, которые краулер найдет по ходу своей работы необходимо записать в файл.

Материалы, которые помогут вам в работе:

* смешная страница про парсинг html через regex и лучший ответ :) https://stackoverflow.com/a/1732454/10012313

* как на самом деле  лучше всего парсить html для получения ссылок: 

  * на английском: https://www.geeksforgeeks.org/extract-all-the-urls-from-the-webpage-using-python/
  
  * на русском: https://overcoder.net/q/51801/%D0%BF%D0%BE%D0%BB%D1%83%D1%87%D0%B8%D1%82%D1%8C-%D1%81%D1%81%D1%8B%D0%BB%D0%BA%D0%B8-%D1%81-%D0%B2%D0%B5%D0%B1-%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B8%D1%86%D1%8B-%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D1%83%D1%8F-python-%D0%B8-beautifulsoup
  
* Документация библиотеки BeautifulSoup https://www.crummy.com/software/BeautifulSoup/bs4/doc.ru/

### Что оценивается:
Реализован рекурсивный краулер внешних ссылок из html с помощью модуля aiohttp, все найденный ссылки сохраняются в файл.
